{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qutrit Machine Learning Optimizaiton "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Bora Basyildiz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from itertools import permutations\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous Function Defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fidelity_ml(J,B,M,target_gate,t,N_iter):\n",
    "    #!/usr/bin/env python3\n",
    "    # -*- coding: utf-8 -*-\n",
    "    \"\"\"\n",
    "    Created on Mon Aug 16 4:33 2021\n",
    "\n",
    "    @author: Bora & Alex\n",
    "\n",
    "    This is the original code for Joel's Paper\n",
    "    \"\"\"\n",
    "    \n",
    "    #Pauli Matricies \n",
    "    sx = np.array([[0, 1], [1, 0]])\n",
    "    sy = np.array([[0,-1j],[1j,0]])\n",
    "    sz = np.array([[1, 0], [0, -1]])\n",
    "    id = np.array([[1,0],[0,1]])\n",
    "    \n",
    "    #Function definitions \n",
    "    def zero_mat(N):#Generates matrix of zeros\n",
    "        zero_gate = np.array([[0,0],[0,0]])\n",
    "        init = np.array([[0,0],[0,0]])\n",
    "        if N < 2:\n",
    "            return 1\n",
    "        for i in range(0,N - 1):\n",
    "            zero_gate = torch.tensor(np.kron(zero_gate,init))\n",
    "        return zero_gate\n",
    "    def sum_pauli(coef, gate):#Sums Pauli gates with coefficients \n",
    "        N = len(coef)#number of qubits\n",
    "        total_pauli = zero_mat(N)\n",
    "        #Summing all Z gates\n",
    "        for i in range(0,N):\n",
    "            pauli_temp = 1\n",
    "            for j in range(0,i):\n",
    "                pauli_temp = torch.tensor(np.kron(pauli_temp,id))\n",
    "            pauli_temp = torch.tensor(np.kron(pauli_temp,gate))\n",
    "            for j in range(i+1,N):\n",
    "                pauli_temp = torch.tensor(np.kron(pauli_temp,id))\n",
    "            total_pauli = total_pauli + coef[i]*pauli_temp\n",
    "        return total_pauli\n",
    "\n",
    "    #variable initializations\n",
    "    N = len(B)\n",
    "    torch.manual_seed(random.randint(0,1000))\n",
    "    dt = torch.cdouble # datatype and precision\n",
    "    infidelity_list=torch.zeros([N_iter,1])\n",
    "\n",
    "    #J coefficients gathering (only if J is in N x N matrix, otherwise set J_coef=J)\n",
    "    J_coef = []\n",
    "    for i in range(0,len(J) - 1):\n",
    "        for j in range(0,len(J) - i - 1):\n",
    "            J_coef.append(J[i,j].item())\n",
    "\n",
    "    #H0 generation\n",
    "    permuts = [1,1]\n",
    "    for i in range(2,N):\n",
    "        permuts.append(0)\n",
    "    permuts = list(set(permutations(permuts,N)))\n",
    "    permuts.sort()\n",
    "    permuts.reverse()#All permutations of ZZ coupling stored as bit arrays\n",
    "    H0 = zero_mat(N)\n",
    "    for i,u in enumerate(permuts):#summing ZZ permutations and J constants\n",
    "        ZZ_temp = 1\n",
    "        for p in u:\n",
    "            if p==1:\n",
    "                ZZ_temp = torch.tensor(np.kron(ZZ_temp,sz))\n",
    "            else:\n",
    "                ZZ_temp = torch.tensor(np.kron(ZZ_temp,id))\n",
    "        H0 = H0 + J_coef[i]*ZZ_temp\n",
    "\n",
    "    #H0 = H0 + sum_pauli(B,sz)\n",
    "\n",
    "    #Unitary group generation\n",
    "    SU = []\n",
    "    pauli_int = [1, 2, 3, 4]#eq to [sx,sy,sz,id]\n",
    "    perms = list(product(pauli_int,repeat=N))#all permutations of paulis\n",
    "    for p in perms:#mapping integers to pauli \n",
    "        unitary = 1\n",
    "        for pauli in p:\n",
    "            if pauli == 1:\n",
    "                unitary = torch.tensor(np.kron(unitary,sx),dtype=torch.cdouble)\n",
    "            elif pauli == 2:\n",
    "                unitary = torch.tensor(np.kron(unitary,sy),dtype=torch.cdouble)\n",
    "            elif pauli == 3:\n",
    "                unitary = torch.tensor(np.kron(unitary,sz),dtype=torch.cdouble)\n",
    "            elif pauli == 4:\n",
    "                unitary = torch.tensor(np.kron(unitary,id),dtype=torch.cdouble)\n",
    "        SU.append(unitary)\n",
    "\n",
    "    #These are the coefficients we are optimizing\n",
    "    R = torch.rand([M,2*N], dtype=torch.double) *2*np.pi # Random initialization (between 0 and 2pi)\n",
    "    R.requires_grad = True # set flag so we can backpropagate\n",
    "\n",
    "    #Optimizer settings(can be changed & opttimized)\n",
    "    lr=0.3#learning rate\n",
    "\n",
    "    opt = 'SGD'  # Choose optimizer - ADAM, SGD (typical). ADAMW, ADAMax, Adadelta,  \n",
    "                        # Adagrad, Rprop, RMSprop, ASGD, also valid options.     \n",
    "    sched = 'Plateau'  # Choose learning rate scheduler - Plateau, Exponential (typical), Step\n",
    "    \n",
    "    if opt=='ADAM': optimizer = torch.optim.Adam([R], lr = lr, weight_decay=1e-6)\n",
    "    elif opt=='ADAMW': optimizer = torch.optim.AdamW([R], lr = lr, weight_decay=0.01)\n",
    "    elif opt=='ADAMax': optimizer = torch.optim.Adamax([R], lr = lr, weight_decay=0.01)\n",
    "    elif opt=='RMSprop': optimizer = torch.optim.RMSprop([R], lr = lr, momentum=0.2)\n",
    "    elif opt=='Rprop': optimizer = torch.optim.Rprop([R], lr = lr)\n",
    "    elif opt=='Adadelta': optimizer = torch.optim.Adadelta([R], lr = lr) \n",
    "    elif opt=='Adagrad': optimizer = torch.optim.Adagrad([R], lr = lr)\n",
    "    elif opt=='SGD': optimizer = torch.optim.SGD([R], lr = lr, momentum=0.99, nesterov=True)\n",
    "    elif opt=='ASGD': optimizer = torch.optim.ASGD([R], lr = lr)\n",
    "    else: optimizer=None; opt='None'\n",
    "        \n",
    "    if sched=='Step': scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=N_iter/10, gamma=0.9)\n",
    "    elif sched=='Exponential': scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    elif sched=='Plateau': scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',min_lr=0.03, factor=0.3 , patience= 20 ); loss_in=True; \n",
    "    else: scheduler=None; sched='None'\n",
    "\n",
    "    for n in range(0,N_iter):\n",
    "        #Creating Hamilontian\n",
    "        U_Exp = 1\n",
    "        for i in range(0,N):\n",
    "            U_Exp = torch.tensor(np.kron(U_Exp,id),dtype=dt)#initializing unitary\n",
    "        for m in range(0,M):#Product of pulses\n",
    "            pulse_coef = R[m]\n",
    "            H1 = sum_pauli(pulse_coef[:N],sx) + sum_pauli(pulse_coef[N:],sy)\n",
    "            U_Exp = torch.matmul(torch.matrix_exp(-1j*(H0+H1)*t/M),U_Exp)\n",
    "\n",
    "        #Fidelity calulcation given by Nielsen Paper\n",
    "        fidelity = 0\n",
    "        d = 2**N\n",
    "        for i in range(0,len(SU)):\n",
    "            eps_U = torch.matmul(torch.matmul(U_Exp,SU[i]),(U_Exp.conj().T))\n",
    "            target_U = torch.matmul(torch.matmul(target_gate,(SU[i].conj().T)),(target_gate.conj().T))\n",
    "            tr = torch.trace(torch.matmul(target_U,eps_U))\n",
    "            fidelity = fidelity + tr\n",
    "        fidelity = abs(fidelity + d*d)/(d*d*(d+1))    \n",
    "        infidelity = 1 - fidelity\n",
    "        infidelity_list[n] = infidelity.detach()\n",
    "        infidelity.backward()\n",
    "\n",
    "        #Printing statement\n",
    "        #if (n+1)%20==0: \n",
    "            #print('Itertation ', str(n+1), ' out of ', str(N_iter), 'complete. Avg Infidelity: ', str(infidelity.item()))\n",
    "\n",
    "        #optimizer \n",
    "        if optimizer is not None and scheduler is None:  # Update R\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        elif optimizer is not None and scheduler is not None:\n",
    "            optimizer.step()\n",
    "            if loss_in: \n",
    "                scheduler.step(infidelity)\n",
    "            else: \n",
    "                scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        else:\n",
    "            R.data.sub_(lr*R.grad.data) # using data avoids overwriting tensor object\n",
    "            R.grad.data.zero_()           # and it's respective grad info\n",
    "    \n",
    "    #print('The infidelity of the generated gate is: ' + str(infidelity_list.min().item()))\n",
    "    #return R\n",
    "    return infidelity_list.min().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fidelity_XX(J,B,M,input_gate,t,N_iter):\n",
    "    #!/usr/bin/env python3\n",
    "    # -*- coding: utf-8 -*-\n",
    "    \"\"\"\n",
    "    Created on Mon Aug 16 4:33 2021\n",
    "\n",
    "    @author: Bora & Alex\n",
    "    \"\"\"\n",
    "    #imports\n",
    "\n",
    "    #Pauli Matricies \n",
    "    sx = np.array([[0, 1], [1, 0]])\n",
    "    sy = np.array([[0,-1j],[1j,0]])\n",
    "    sz = np.array([[1, 0], [0, -1]])\n",
    "    id = np.array([[1,0],[0,1]])\n",
    "    \n",
    "    #Function definitions \n",
    "    def zero_mat(N):#Generates matrix of zeros\n",
    "        zero_gate = np.array([[0,0],[0,0]])\n",
    "        init = np.array([[0,0],[0,0]])\n",
    "        if N < 2:\n",
    "            return 1\n",
    "        for i in range(0,N - 1):\n",
    "            zero_gate = torch.tensor(np.kron(zero_gate,init))\n",
    "        return zero_gate\n",
    "    def sum_pauli(coef, gate):#Sums Pauli gates with coefficients \n",
    "        N = len(coef)#number of qubits\n",
    "        total_pauli = zero_mat(N)\n",
    "        #Summing all Z gates\n",
    "        for i in range(0,N):\n",
    "            pauli_temp = 1\n",
    "            for j in range(0,i):\n",
    "                pauli_temp = torch.tensor(np.kron(pauli_temp,id))\n",
    "            pauli_temp = torch.tensor(np.kron(pauli_temp,gate))\n",
    "            for j in range(i+1,N):\n",
    "                pauli_temp = torch.tensor(np.kron(pauli_temp,id))\n",
    "            total_pauli = total_pauli + coef[i]*pauli_temp\n",
    "        return total_pauli\n",
    "\n",
    "    #variable initializations\n",
    "    N = len(B)\n",
    "    torch.manual_seed(random.randint(0,1000))\n",
    "    dt = torch.cdouble # datatype and precision\n",
    "    infidelity_list=torch.zeros([N_iter,1])\n",
    "\n",
    "    #J coefficients gathering (only if J is in N x N matrix, otherwise set J_coef=J)\n",
    "    J_coef = []\n",
    "    for i in range(0,len(J) - 1):\n",
    "        for j in range(0,len(J) - i - 1):\n",
    "            J_coef.append(J[i,j].item())\n",
    "\n",
    "    #H0 generation\n",
    "    permuts = [1,1]\n",
    "    for i in range(2,N):\n",
    "        permuts.append(0)\n",
    "    permuts = list(set(permutations(permuts,N)))\n",
    "    permuts.sort()\n",
    "    permuts.reverse()#All permutations of ZZ coupling stored as bit arrays\n",
    "    H0 = zero_mat(N)\n",
    "    ##Changed gates ZZ -> XX\n",
    "    for i,u in enumerate(permuts):#summing ZZ permutations and J constants\n",
    "        XX_temp = 1\n",
    "        for p in u:\n",
    "            if p==1:\n",
    "                XX_temp = torch.tensor(np.kron(XX_temp,sx))\n",
    "            else:\n",
    "                XX_temp = torch.tensor(np.kron(XX_temp,id))\n",
    "        H0 = H0 + J_coef[i]*XX_temp\n",
    "\n",
    "    #H0 = H0 + sum_pauli(B,sz)\n",
    "    #print(H0) <- checking if the H0 is a XX gate \n",
    "\n",
    "    #Unitary group generation\n",
    "    SU = []\n",
    "    pauli_int = [1, 2, 3, 4]#eq to [sx,sy,sz,id]\n",
    "    perms = list(product(pauli_int,repeat=N))#all permutations of paulis\n",
    "    for p in perms:#mapping integers to pauli \n",
    "        unitary = 1\n",
    "        for pauli in p:\n",
    "            if pauli == 1:\n",
    "                unitary = torch.tensor(np.kron(unitary,sx),dtype=torch.cdouble)\n",
    "            elif pauli == 2:\n",
    "                unitary = torch.tensor(np.kron(unitary,sy),dtype=torch.cdouble)\n",
    "            elif pauli == 3:\n",
    "                unitary = torch.tensor(np.kron(unitary,sz),dtype=torch.cdouble)\n",
    "            elif pauli == 4:\n",
    "                unitary = torch.tensor(np.kron(unitary,id),dtype=torch.cdouble)\n",
    "        SU.append(unitary)\n",
    "\n",
    "    #These are the coefficients we are optimizing\n",
    "    R = torch.rand([M,2*N], dtype=torch.double) *2*np.pi # Random initialization (between 0 and 2pi)\n",
    "    R.requires_grad = True # set flag so we can backpropagate\n",
    "\n",
    "    #Optimizer settings(can be changed & opttimized)\n",
    "    lr=0.3#learning rate\n",
    "\n",
    "    opt = 'SGD'  # Choose optimizer - ADAM, SGD (typical). ADAMW, ADAMax, Adadelta,  \n",
    "                        # Adagrad, Rprop, RMSprop, ASGD, also valid options.     \n",
    "    sched = 'Plateau'  # Choose learning rate scheduler - Plateau, Exponential (typical), Step\n",
    "    \n",
    "    if opt=='ADAM': optimizer = torch.optim.Adam([R], lr = lr, weight_decay=1e-6)\n",
    "    elif opt=='ADAMW': optimizer = torch.optim.AdamW([R], lr = lr, weight_decay=0.01)\n",
    "    elif opt=='ADAMax': optimizer = torch.optim.Adamax([R], lr = lr, weight_decay=0.01)\n",
    "    elif opt=='RMSprop': optimizer = torch.optim.RMSprop([R], lr = lr, momentum=0.2)\n",
    "    elif opt=='Rprop': optimizer = torch.optim.Rprop([R], lr = lr)\n",
    "    elif opt=='Adadelta': optimizer = torch.optim.Adadelta([R], lr = lr) \n",
    "    elif opt=='Adagrad': optimizer = torch.optim.Adagrad([R], lr = lr)\n",
    "    elif opt=='SGD': optimizer = torch.optim.SGD([R], lr = lr, momentum=0.99, nesterov=True)\n",
    "    elif opt=='ASGD': optimizer = torch.optim.ASGD([R], lr = lr)\n",
    "    else: optimizer=None; opt='None'\n",
    "        \n",
    "    if sched=='Step': scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=N_iter/10, gamma=0.9)\n",
    "    elif sched=='Exponential': scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    elif sched=='Plateau': scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',min_lr=0.03, factor=0.3 , patience= 20 ); loss_in=True; \n",
    "    else: scheduler=None; sched='None'\n",
    "\n",
    "    for n in range(0,N_iter):\n",
    "        #Creating Hamilontian\n",
    "        U_Exp = 1\n",
    "        for i in range(0,N):\n",
    "            U_Exp = torch.tensor(np.kron(U_Exp,id),dtype=dt)#initializing unitary\n",
    "        for m in range(0,M):#Product of pulses\n",
    "            pulse_coef = R[m]\n",
    "            H1 = sum_pauli(pulse_coef[:N],sx) + sum_pauli(pulse_coef[N:],sy)\n",
    "            U_Exp = torch.matmul(torch.matrix_exp(-1j*(H0+H1)*t/M),U_Exp)\n",
    "\n",
    "        #Fidelity calulcation given by Nielsen Paper\n",
    "        fidelity = 0\n",
    "        d = 2**N\n",
    "        for i in range(0,len(SU)):\n",
    "            ideal_U = torch.matmul(torch.matmul(input_gate,SU[i]),(input_gate.conj().T))\n",
    "            target_U = torch.matmul(torch.matmul(U_Exp,SU[i]),(U_Exp.conj().T)) # This is Eps(U) = pulse_gate * pauli * pulse_gate^H\n",
    "            tr = torch.trace(torch.matmul(ideal_U,target_U))\n",
    "            fidelity = fidelity + tr\n",
    "        fidelity = abs(fidelity + d*d)/(d*d*(d+1))    \n",
    "        infidelity = 1 - fidelity\n",
    "        infidelity_list[n] = infidelity.detach()\n",
    "        infidelity.backward()\n",
    "\n",
    "        #Printing statement\n",
    "        #if (n+1)%100==0: \n",
    "            #print('Itertation ', str(n+1), ' out of ', str(N_iter), 'complete. Avg Infidelity: ', str(infidelity.item()))\n",
    "\n",
    "        #optimizer \n",
    "        if optimizer is not None and scheduler is None:  # Update R\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        elif optimizer is not None and scheduler is not None:\n",
    "            optimizer.step()\n",
    "            if loss_in: \n",
    "                scheduler.step(infidelity)\n",
    "            else: \n",
    "                scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        else:\n",
    "            R.data.sub_(lr*R.grad.data) # using data avoids overwriting tensor object\n",
    "            R.grad.data.zero_()           # and it's respective grad info\n",
    "    \n",
    "    #print('The infidelity of the generated gate is: ' + str(infidelity_list.min().item()))\n",
    "    #return R\n",
    "    return infidelity_list.min().item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qutrit Function Defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fidelity_Qutrit(J,B,M,input_gate,t,N_iter):\n",
    "    #!/usr/bin/env python3\n",
    "    # -*- coding: utf-8 -*-\n",
    "    \"\"\"\n",
    "    Created on Mon Aug 16 4:33 2021\n",
    "\n",
    "    @author: Bora & Alex\n",
    "    \"\"\"\n",
    "    #imports\n",
    "\n",
    "    #Pauli Matricies \n",
    "    l1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) \n",
    "    l2 = np.array([[0,-1j, 0],[1j,0, 0], [0, 0, 0]]) \n",
    "    l3 = np.array([[1, 0, 0], [0, -1, 0], [0, 0, 0]]) \n",
    "    l4 = np.array([[0,0,1],[0,0,0],[1,0,0]])\n",
    "    l5 = np.array([[0,0,-1j],[0,0,0],[1j,0,0]])\n",
    "    l6 = np.array([[0,0,0],[0,0,1],[0,1,0]]) #essentially X for 1->2 transition\n",
    "    l7 = np.array([[0,0,0],[0,0,-1j],[0,1j,0]]) #essentially Y for 1->2 transition\n",
    "    l8 = 1/np.sqrt(3) * np.array([[1,0,0],[0,1,0],[0,0,-2]])\n",
    "    id = np.array([[1, 0, 0],[0, 1, 0],[0, 0, 1]])\n",
    "\n",
    "    sx = np.array([[0,1,0],[1,0,0],[0,0,1]])\n",
    "    sy = np.array([[0,-1j,0],[1j,0,0],[0,0,1]])\n",
    "    sx2 = np.array([[1,0,0],[0,0,1],[0,1,0]])\n",
    "    sy2 = np.array([[1,0,0],[0,0,-1j],[0,1j,0]])\n",
    "    sx02 = np.array([[0,0,1],[0,1,0],[1,0,0]])\n",
    "    sy02 = np.array([[0,0,-1j],[0,1,0],[1j,0,0]])\n",
    "    sz = np.array([[1,0,0],[0,-1,0],[0,0,1]])\n",
    "    \n",
    "    #Function definitions \n",
    "    def zero_mat(N):#Generates matrix of zeros\n",
    "        zero_gate = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
    "        init = zero_gate\n",
    "        if N < 2:\n",
    "            return 1\n",
    "        for i in range(0,N - 1):\n",
    "            zero_gate = torch.tensor(np.kron(zero_gate,init),dtype=torch.cdouble)\n",
    "        return zero_gate\n",
    "    def sum_pauli(coef, gate):#Sums Pauli gates with coefficients \n",
    "        N = len(coef)#number of qubits\n",
    "        total_pauli = zero_mat(N)\n",
    "        #Summing all Z gates\n",
    "        for i in range(0,N):\n",
    "            pauli_temp = 1\n",
    "            for j in range(0,i):\n",
    "                pauli_temp = torch.tensor(np.kron(pauli_temp,id))\n",
    "            pauli_temp = torch.tensor(np.kron(pauli_temp,gate))\n",
    "            for j in range(i+1,N):\n",
    "                pauli_temp = torch.tensor(np.kron(pauli_temp,id))\n",
    "            total_pauli = total_pauli + coef[i]*pauli_temp\n",
    "        #return torch.tensor(total_pauli,dtype=torch.cdouble)\n",
    "        return total_pauli\n",
    "\n",
    "    #variable initializations\n",
    "    N = len(B)\n",
    "    torch.manual_seed(1)\n",
    "    dt = torch.cdouble # datatype and precision\n",
    "    infidelity_list=torch.zeros([N_iter,1])\n",
    "\n",
    "    #J coefficients gathering (only if J is in N x N matrix, otherwise set J_coef=J) <- essentially flattens the array\n",
    "    J_coef = []\n",
    "    for i in range(0,len(J) - 1):\n",
    "        for j in range(0,len(J) - i - 1):\n",
    "            J_coef.append(J[i,j].item())\n",
    "\n",
    "    #H0 generation\n",
    "    permuts = [1,1]\n",
    "    for i in range(2,N):\n",
    "        permuts.append(0)\n",
    "    permuts = list(set(permutations(permuts,N)))\n",
    "    permuts.sort()\n",
    "    permuts.reverse()#All permutations of ZZ coupling stored as bit arrays\n",
    "    H0 = zero_mat(N)\n",
    "    ##Changed gates ZZ -> XX\n",
    "    for i,u in enumerate(permuts):#summing ZZ permutations and J constants\n",
    "        XX_temp = 1\n",
    "        for p in u:\n",
    "            if p==1:\n",
    "                XX_temp = torch.tensor(np.kron(XX_temp,l1))\n",
    "            else:\n",
    "                XX_temp = torch.tensor(np.kron(XX_temp,id))\n",
    "        H0 = H0 + J_coef[i]*XX_temp\n",
    "    #H0 = H0 + sum_pauli(B,sz)\n",
    "\n",
    "    #These are the coefficients we are optimizing\n",
    "    R = torch.rand([M,4*N], dtype=torch.double) *2*np.pi # Random initialization (between 0 and 2pi)\n",
    "    R.requires_grad = True # set flag so we can backpropagate\n",
    "\n",
    "    #Optimizer settings(can be changed & opttimized)\n",
    "    lr=0.3#learning rate\n",
    "\n",
    "    opt = 'SGD'  # Choose optimizer - ADAM, SGD (typical). ADAMW, ADAMax, Adadelta,  \n",
    "                        # Adagrad, Rprop, RMSprop, ASGD, also valid options.     \n",
    "    sched = 'Plateau'  # Choose learning rate scheduler - Plateau, Exponential (typical), Step\n",
    "    \n",
    "    if opt=='ADAM': optimizer = torch.optim.Adam([R], lr = lr, weight_decay=1e-6)\n",
    "    elif opt=='ADAMW': optimizer = torch.optim.AdamW([R], lr = lr, weight_decay=0.01)\n",
    "    elif opt=='ADAMax': optimizer = torch.optim.Adamax([R], lr = lr, weight_decay=0.01)\n",
    "    elif opt=='RMSprop': optimizer = torch.optim.RMSprop([R], lr = lr, momentum=0.2)\n",
    "    elif opt=='Rprop': optimizer = torch.optim.Rprop([R], lr = lr)\n",
    "    elif opt=='Adadelta': optimizer = torch.optim.Adadelta([R], lr = lr) \n",
    "    elif opt=='Adagrad': optimizer = torch.optim.Adagrad([R], lr = lr)\n",
    "    elif opt=='SGD': optimizer = torch.optim.SGD([R], lr = lr, momentum=0.99, nesterov=True)\n",
    "    elif opt=='ASGD': optimizer = torch.optim.ASGD([R], lr = lr)\n",
    "    else: optimizer=None; opt='None'\n",
    "        \n",
    "    if sched=='Step': scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=N_iter/10, gamma=0.9)\n",
    "    elif sched=='Exponential': scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    elif sched=='Plateau': scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',min_lr=0.03, factor=0.3 , patience= 20 ); loss_in=True; \n",
    "    else: scheduler=None; sched='None'\n",
    "\n",
    "    for n in range(0,N_iter):\n",
    "        #Creating Drive Hamilontian\n",
    "        U_Exp = 1\n",
    "        for i in range(0,N):\n",
    "            U_Exp = torch.tensor(np.kron(U_Exp,id),dtype=dt)#initializing unitary\n",
    "        for m in range(0,M):#Product of pulses\n",
    "            pulse_coef = R[m]\n",
    "            #H1 = sum_pauli(pulse_coef[:N],l1) + sum_pauli(pulse_coef[N:2*N],l2) + sum_pauli(pulse_coef[2*N:3*N],l4) + sum_pauli(pulse_coef[3*N:4*N],l5) + sum_pauli(pulse_coef[4*N:5*N],l6) + sum_pauli(pulse_coef[5*N:],l7) \n",
    "            H1 = sum_pauli(pulse_coef[:N],l1) + sum_pauli(pulse_coef[N:2*N],l2) + sum_pauli(pulse_coef[2*N:3*N],l4) \n",
    "            #H1 = sum_pauli(pulse_coef[:N],l1) + sum_pauli(pulse_coef[N:2*N],l2) \n",
    "            U_Exp = torch.matmul(torch.matrix_exp(-1j*(H0+H1)*t/M),U_Exp)\n",
    "        \n",
    "        #Orthonormal Matrix Generation \n",
    "        def Matrix_Basis_Gen(d):\n",
    "            X = np.zeros([d,d])\n",
    "            for i in range(d):\n",
    "                X[(i+1) % d,i] = 1\n",
    "            Z = np.zeros([d,d],dtype=np.complex_)\n",
    "            for i in range(d):\n",
    "                Z[i,i] = np.exp((2*np.pi * 1j * i)/d)\n",
    "            Basis = []\n",
    "            for i in range(d):\n",
    "                for j in range(d):\n",
    "                    Basis.append(torch.tensor(np.matmul(np.linalg.matrix_power(X,i),np.linalg.matrix_power(Z,j)),dtype=torch.cdouble))\n",
    "            return Basis\n",
    "\n",
    "        #Fidelity calulcation given by Nielsen Paper\n",
    "        fidelity = 0\n",
    "        d = 3**N\n",
    "        \n",
    "        for U in Matrix_Basis_Gen(d):\n",
    "            ideal_U = torch.matmul(torch.matmul(input_gate,U.conj().T),(input_gate.conj().T))\n",
    "            target_U = torch.matmul(torch.matmul(U_Exp,U),(U_Exp.conj().T)) # This is Eps(U) = pulse_gate * pauli * pulse_gate^H\n",
    "            tr = torch.trace(torch.matmul(ideal_U,target_U))\n",
    "            fidelity = fidelity + tr \n",
    "        fidelity = abs(fidelity + d**2)/(d**2 *(d+1))    \n",
    "        infidelity = 1 - fidelity\n",
    "        infidelity_list[n] = infidelity.detach()\n",
    "        infidelity.backward()\n",
    "\n",
    "        #Printing statement\n",
    "        if (n+1)%50==0: \n",
    "            print('Itertation ', str(n+1), ' out of ', str(N_iter), 'complete. Avg Infidelity: ', str(infidelity.item()))\n",
    "\n",
    "        #optimizer \n",
    "        if optimizer is not None and scheduler is None:  # Update R\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        elif optimizer is not None and scheduler is not None:\n",
    "            optimizer.step()\n",
    "            if loss_in: \n",
    "                scheduler.step(infidelity)\n",
    "            else: \n",
    "                scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        else:\n",
    "            R.data.sub_(lr*R.grad.data) # using data avoids overwriting tensor object\n",
    "            R.grad.data.zero_()           # and it's respective grad info\n",
    "    \n",
    "    #print('The infidelity of the generated gate is: ' + str(infidelity_list.min().item()))\n",
    "    return R\n",
    "    #return infidelity_list.min().item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Qutrit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '2 Qubit CNOT Fidelity for ZZ coupled H0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#graph generation for ZZ coupled Hamiltonain\n",
    "CNOT = torch.tensor([[1,0,0,0],[0,1,0,0],[0,0,0,1],[0,0,1,0]],dtype=torch.cdouble)\n",
    "J = torch.tensor([[1,1],[1,1]])\n",
    "\n",
    "Fidelities = []\n",
    "Times = []\n",
    "for i in range(0,31):\n",
    "    Times.append(i/10)\n",
    "    best_fidelity = 0\n",
    "    for j in range(0,10):\n",
    "        temp_fidelity = 1 - fidelity_ml(J,[1,1],16,CNOT,i/10*np.pi/4,1000)\n",
    "        if temp_fidelity > best_fidelity:\n",
    "            best_fidelity = temp_fidelity\n",
    "    Fidelities.append(best_fidelity)\n",
    "\n",
    "plt.plot(Times,Fidelities)\n",
    "plt.xlabel(\"T/Tmin\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.title(\"2 Qubit CNOT Fidelity for ZZ coupled H0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8afbc26004cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbest_fidelity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtemp_fidelity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfidelity_XX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCNOT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtemp_fidelity\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_fidelity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mbest_fidelity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_fidelity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-1d02e7e25dbb>\u001b[0m in \u001b[0;36mfidelity_XX\u001b[0;34m(J, B, M, input_gate, t, N_iter)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0minfidelity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfidelity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0minfidelity_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfidelity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0minfidelity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m#Printing statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py7/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#graph generation for ZZ coupled Hamiltonain\n",
    "CNOT = torch.tensor([[1,0,0,0],[0,1,0,0],[0,0,0,1],[0,0,1,0]],dtype=torch.cdouble)\n",
    "J = torch.tensor([[1,1],[1,1]])\n",
    "\n",
    "Fidelities = []\n",
    "Times = []\n",
    "for i in range(0,31):\n",
    "    Times.append(i/10)\n",
    "    best_fidelity = 0\n",
    "    for j in range(0,10):\n",
    "        temp_fidelity = 1 - fidelity_XX(J,[1,1],16,CNOT,i/10*np.pi/4,1000)\n",
    "        if temp_fidelity > best_fidelity:\n",
    "            best_fidelity = temp_fidelity\n",
    "    Fidelities.append(best_fidelity)\n",
    "\n",
    "plt.plot(Times,Fidelities)\n",
    "plt.xlabel(\"T/Tmin\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.title(\"2 Qubit CNOT Fidelity for XX coupled H0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph generation for XX coupled Hamiltonain\n",
    "CNOT_qutrit = torch.tensor([[1,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0,0],[0,0,0,1,0,0,0,0,0],[0,0,1,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],\n",
    "[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0]],dtype=torch.cdouble)\n",
    "J = torch.tensor([[1,1],[1,1]])\n",
    "\n",
    "Fidelities = []\n",
    "Times = []\n",
    "for i in range(0,31):\n",
    "    Times.append(i/10)\n",
    "    Fidelities.append(1 - fidelity_XX(J,[1,1],16,CNOT,i/10*np.pi/4,1000))\n",
    "\n",
    "plt.plot(Times,Fidelities)\n",
    "plt.xlabel(\"T/Tmin\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.title(\"2 Qubit CNOT Fidelity for XX coupled H0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNOT_qutrit = torch.tensor([[1,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,1,0,0,0,0],[0,0,0,1,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],\n",
    "[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0]],dtype=torch.cdouble)\n",
    "J = torch.tensor([[1,1],[1,1]])\n",
    "fidelity_Qutrit(J,[1,1],32,CNOT_qutrit,10*np.pi,200)\n",
    "\n",
    "Fidelities = []\n",
    "Times = []\n",
    "for i in range(0,31):\n",
    "    Times.append(i/10)\n",
    "    Fidelities.append(1 - fidelity_Qutrit(J,[1,1],16,CNOT_qutrit,i/10*np.pi/4,500))\n",
    "\n",
    "plt.plot(Times,Fidelities)\n",
    "plt.xlabel(\"T/Tmin\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.title(\"2 Qubit CNOT Fidelity for XX coupled Qutrit H0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itertation  20  out of  1000 complete. Avg Infidelity:  0.8833275397898315\n",
      "Itertation  40  out of  1000 complete. Avg Infidelity:  0.8830224065577897\n",
      "Itertation  60  out of  1000 complete. Avg Infidelity:  0.8825532105317029\n",
      "Itertation  80  out of  1000 complete. Avg Infidelity:  0.881943204055911\n",
      "Itertation  100  out of  1000 complete. Avg Infidelity:  0.8812154639972397\n",
      "Itertation  120  out of  1000 complete. Avg Infidelity:  0.8803944323145223\n",
      "Itertation  140  out of  1000 complete. Avg Infidelity:  0.8795035617404974\n",
      "Itertation  160  out of  1000 complete. Avg Infidelity:  0.8785588480811006\n",
      "Itertation  180  out of  1000 complete. Avg Infidelity:  0.8775600290546578\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e72c9b93731a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m [0,0,0,0,0,0,1,0,0],[0,0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,0,1]],dtype=torch.cdouble)\n\u001b[1;32m      3\u001b[0m \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfidelity_Qutrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIdentity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-fe2ef8b52383>\u001b[0m in \u001b[0;36mfidelity_Qutrit\u001b[0;34m(J, B, M, target_gate, t, N_iter)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0minfidelity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfidelity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0minfidelity_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfidelity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0minfidelity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m#Printing statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py7/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Identity = torch.tensor([[1,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0,0],[0,0,1,0,0,0,0,0,0],[0,0,0,0,1,0,0,0,0],[0,0,0,1,0,0,0,0,0],[0,0,0,0,0,1,0,0,0],\n",
    "[0,0,0,0,0,0,1,0,0],[0,0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,0,1]],dtype=torch.cdouble)\n",
    "J = torch.tensor([[1,1],[1,1]])\n",
    "fidelity_Qutrit(J,[1,1],32,Identity,np.pi/4,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "        [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "        [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n",
       "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n",
       "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]],\n",
       "       dtype=torch.complex128)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNOT_qutrit.T * CNOT_qutrit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itertation  20  out of  200 complete. Avg Infidelity:  0.8118579802397738\n",
      "Itertation  40  out of  200 complete. Avg Infidelity:  0.8082186603551773\n",
      "Itertation  60  out of  200 complete. Avg Infidelity:  0.8074939484808253\n",
      "Itertation  80  out of  200 complete. Avg Infidelity:  0.8073040874940538\n",
      "Itertation  100  out of  200 complete. Avg Infidelity:  0.8072776392452026\n",
      "Itertation  120  out of  200 complete. Avg Infidelity:  0.8072856978763698\n",
      "Itertation  140  out of  200 complete. Avg Infidelity:  0.807271588421193\n",
      "Itertation  160  out of  200 complete. Avg Infidelity:  0.80727785596674\n",
      "Itertation  180  out of  200 complete. Avg Infidelity:  0.807272408124901\n",
      "Itertation  200  out of  200 complete. Avg Infidelity:  0.8072729670784975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4276,  5.5662,  2.2977,  0.6540],\n",
       "        [-0.0849,  4.1260,  0.7485,  0.9280],\n",
       "        [ 0.8269,  3.8214,  3.1729,  1.6616],\n",
       "        [ 3.5604,  2.8295,  4.8471,  0.4763],\n",
       "        [ 0.4893,  5.0739,  6.0796,  3.4469],\n",
       "        [ 3.7098,  0.9607,  2.3485,  4.2243],\n",
       "        [ 1.1343,  0.3738,  5.1079,  2.8282],\n",
       "        [ 0.7442,  2.5203,  1.6716,  6.3206],\n",
       "        [ 1.7282,  6.0637,  5.5998,  4.6028],\n",
       "        [-0.2751,  5.9062,  2.6022,  2.0082],\n",
       "        [ 2.1163,  2.2922,  5.6663, -0.3232],\n",
       "        [ 4.1946,  4.5616,  2.1886,  5.3798],\n",
       "        [ 5.4878,  5.1738,  1.9315,  4.9067],\n",
       "        [ 3.3785,  4.4344,  6.9167,  1.7313],\n",
       "        [ 1.6522,  0.2232,  0.3475,  5.7342],\n",
       "        [ 1.3610,  5.8410,  4.2447,  2.0582],\n",
       "        [ 4.1184,  5.3460,  1.3447,  5.4545],\n",
       "        [ 0.6373,  3.5061,  0.9662,  3.8482],\n",
       "        [ 5.1487,  4.1360,  4.6240,  0.9626],\n",
       "        [ 4.9523,  5.1393,  2.4269,  4.4603],\n",
       "        [ 4.7738,  1.0045,  4.8024,  1.3207],\n",
       "        [ 2.8337,  1.3251,  2.2948,  2.8190],\n",
       "        [ 1.4185,  5.3817, -0.2515,  3.1702],\n",
       "        [ 0.3993,  4.8470,  3.1124,  4.9073],\n",
       "        [ 1.5159,  0.7771,  4.0592,  2.8309],\n",
       "        [ 0.3998,  3.6357,  2.3372,  4.8556],\n",
       "        [ 3.1666,  2.8902,  4.6054,  2.4381],\n",
       "        [ 3.7490,  1.7985,  5.1758,  1.6669],\n",
       "        [ 5.0917,  4.1604,  3.8203,  2.6608],\n",
       "        [ 3.4258, -0.4883,  4.5790,  0.9192],\n",
       "        [ 6.1392,  2.3204,  0.3517,  0.9466],\n",
       "        [ 4.0070,  2.1774,  2.7009,  5.9847]], dtype=torch.float64,\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNOT_qutrit = torch.tensor([[1,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0,0],[0,0,1,0,0,0,0,0,0],[0,0,0,1,0,0,0,0,0],[0,0,0,0,1,0,0,0,0],[0,0,0,0,0,1,0,0,0],\n",
    "[0,0,0,0,0,0,1,0,0],[0,0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,0,1]],dtype=torch.cdouble)\n",
    "J = torch.tensor([[1,1],[1,1]])\n",
    "fidelity_Qutrit(J,[1,1],32,CNOT_qutrit,10*np.pi,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a matrix basis that is orthonormal AND unitary. While the Gell-Mann matrices are orthonormal, they are not unitary. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we will apply a phase of i on specific matrices so that we span the complex plane "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will generate these matrices through the method outlined by the Nielsen paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_Gen(d):\n",
    "    X = np.zeros([d,d])\n",
    "    for i in range(d):\n",
    "        X[(i+1) % d,i] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z_Gen(d):\n",
    "    Z = np.zeros([d,d],dtype=np.complex_)\n",
    "    for i in range(d):\n",
    "        Z[i,i] = np.exp((2*np.pi * 1j * i)/d)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Matrix_Basis_Gen(d):\n",
    "    X = np.zeros([d,d])\n",
    "    for i in range(d):\n",
    "        X[(i+1) % d,i] = 1\n",
    "    Z = np.zeros([d,d],dtype=np.complex_)\n",
    "    for i in range(d):\n",
    "        Z[i,i] = np.exp((2*np.pi * 1j * i)/d)\n",
    "    Basis = []\n",
    "    for i in range(d):\n",
    "        for j in range(d):\n",
    "            Basis.append(np.matmul(np.linalg.matrix_power(X,i),np.linalg.matrix_power(Z,j)))\n",
    "    return Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.+0.j, 0.+0.j],\n",
       "        [0.+0.j, 1.+0.j]]),\n",
       " array([[ 1.+0.0000000e+00j,  0.+0.0000000e+00j],\n",
       "        [ 0.+0.0000000e+00j, -1.+1.2246468e-16j]]),\n",
       " array([[0.+0.j, 1.+0.j],\n",
       "        [1.+0.j, 0.+0.j]]),\n",
       " array([[ 0.+0.0000000e+00j, -1.+1.2246468e-16j],\n",
       "        [ 1.+0.0000000e+00j,  0.+0.0000000e+00j]])]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matrix_Basis_Gen(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "efa8cc2a4d6f65357a972a944fbd3e02e547e4ca88e36cc716fc5e5cc822571e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
