{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qutrit Machine Learning Optimizaiton "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Bora Basyildiz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from itertools import permutations\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous Function Defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fidelity_ml(J,B,M,target_gate,t,N_iter):\n",
    "    #!/usr/bin/env python3\n",
    "    # -*- coding: utf-8 -*-\n",
    "    \"\"\"\n",
    "    Created on Mon Aug 16 4:33 2021\n",
    "\n",
    "    @author: Bora & Alex\n",
    "\n",
    "    This is the original code for Joel's Paper\n",
    "    \"\"\"\n",
    "    \n",
    "    #Pauli Matricies \n",
    "    sx = np.array([[0, 1], [1, 0]])\n",
    "    sy = np.array([[0,-1j],[1j,0]])\n",
    "    sz = np.array([[1, 0], [0, -1]])\n",
    "    id = np.array([[1,0],[0,1]])\n",
    "    \n",
    "    #Function definitions \n",
    "    def zero_mat(N):#Generates matrix of zeros\n",
    "        zero_gate = np.array([[0,0],[0,0]])\n",
    "        init = np.array([[0,0],[0,0]])\n",
    "        if N < 2:\n",
    "            return 1\n",
    "        for i in range(0,N - 1):\n",
    "            zero_gate = torch.tensor(np.kron(zero_gate,init))\n",
    "        return zero_gate\n",
    "    def sum_pauli(coef, gate):#Sums Pauli gates with coefficients \n",
    "        N = len(coef)#number of qubits\n",
    "        total_pauli = zero_mat(N)\n",
    "        #Summing all Z gates\n",
    "        for i in range(0,N):\n",
    "            pauli_temp = 1\n",
    "            for j in range(0,i):\n",
    "                pauli_temp = torch.tensor(np.kron(pauli_temp,id))\n",
    "            pauli_temp = torch.tensor(np.kron(pauli_temp,gate))\n",
    "            for j in range(i+1,N):\n",
    "                pauli_temp = torch.tensor(np.kron(pauli_temp,id))\n",
    "            total_pauli = total_pauli + coef[i]*pauli_temp\n",
    "        return total_pauli\n",
    "\n",
    "    #variable initializations\n",
    "    N = len(B)\n",
    "    torch.manual_seed(9)\n",
    "    dt = torch.cdouble # datatype and precision\n",
    "    infidelity_list=torch.zeros([N_iter,1])\n",
    "\n",
    "    #J coefficients gathering (only if J is in N x N matrix, otherwise set J_coef=J)\n",
    "    J_coef = []\n",
    "    for i in range(0,len(J) - 1):\n",
    "        for j in range(0,len(J) - i - 1):\n",
    "            J_coef.append(J[i,j].item())\n",
    "\n",
    "    #H0 generation\n",
    "    permuts = [1,1]\n",
    "    for i in range(2,N):\n",
    "        permuts.append(0)\n",
    "    permuts = list(set(permutations(permuts,N)))\n",
    "    permuts.sort()\n",
    "    permuts.reverse()#All permutations of ZZ coupling stored as bit arrays\n",
    "    H0 = zero_mat(N)\n",
    "    for i,u in enumerate(permuts):#summing ZZ permutations and J constants\n",
    "        ZZ_temp = 1\n",
    "        for p in u:\n",
    "            if p==1:\n",
    "                ZZ_temp = torch.tensor(np.kron(ZZ_temp,sz))\n",
    "            else:\n",
    "                ZZ_temp = torch.tensor(np.kron(ZZ_temp,id))\n",
    "        H0 = H0 + J_coef[i]*ZZ_temp\n",
    "\n",
    "    #H0 = H0 + sum_pauli(B,sz)\n",
    "\n",
    "    #Unitary group generation\n",
    "    SU = []\n",
    "    pauli_int = [1, 2, 3, 4]#eq to [sx,sy,sz,id]\n",
    "    perms = list(product(pauli_int,repeat=N))#all permutations of paulis\n",
    "    for p in perms:#mapping integers to pauli \n",
    "        unitary = 1\n",
    "        for pauli in p:\n",
    "            if pauli == 1:\n",
    "                unitary = torch.tensor(np.kron(unitary,sx),dtype=torch.cdouble)\n",
    "            elif pauli == 2:\n",
    "                unitary = torch.tensor(np.kron(unitary,sy),dtype=torch.cdouble)\n",
    "            elif pauli == 3:\n",
    "                unitary = torch.tensor(np.kron(unitary,sz),dtype=torch.cdouble)\n",
    "            elif pauli == 4:\n",
    "                unitary = torch.tensor(np.kron(unitary,id),dtype=torch.cdouble)\n",
    "        SU.append(unitary)\n",
    "\n",
    "    #These are the coefficients we are optimizing\n",
    "    R = torch.rand([M,2*N], dtype=torch.double) *2*np.pi # Random initialization (between 0 and 2pi)\n",
    "    R.requires_grad = True # set flag so we can backpropagate\n",
    "\n",
    "    #Optimizer settings(can be changed & opttimized)\n",
    "    lr=0.3#learning rate\n",
    "\n",
    "    opt = 'SGD'  # Choose optimizer - ADAM, SGD (typical). ADAMW, ADAMax, Adadelta,  \n",
    "                        # Adagrad, Rprop, RMSprop, ASGD, also valid options.     \n",
    "    sched = 'Plateau'  # Choose learning rate scheduler - Plateau, Exponential (typical), Step\n",
    "    \n",
    "    if opt=='ADAM': optimizer = torch.optim.Adam([R], lr = lr, weight_decay=1e-6)\n",
    "    elif opt=='ADAMW': optimizer = torch.optim.AdamW([R], lr = lr, weight_decay=0.01)\n",
    "    elif opt=='ADAMax': optimizer = torch.optim.Adamax([R], lr = lr, weight_decay=0.01)\n",
    "    elif opt=='RMSprop': optimizer = torch.optim.RMSprop([R], lr = lr, momentum=0.2)\n",
    "    elif opt=='Rprop': optimizer = torch.optim.Rprop([R], lr = lr)\n",
    "    elif opt=='Adadelta': optimizer = torch.optim.Adadelta([R], lr = lr) \n",
    "    elif opt=='Adagrad': optimizer = torch.optim.Adagrad([R], lr = lr)\n",
    "    elif opt=='SGD': optimizer = torch.optim.SGD([R], lr = lr, momentum=0.99, nesterov=True)\n",
    "    elif opt=='ASGD': optimizer = torch.optim.ASGD([R], lr = lr)\n",
    "    else: optimizer=None; opt='None'\n",
    "        \n",
    "    if sched=='Step': scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=N_iter/10, gamma=0.9)\n",
    "    elif sched=='Exponential': scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    elif sched=='Plateau': scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',min_lr=0.03, factor=0.3 , patience= 20 ); loss_in=True; \n",
    "    else: scheduler=None; sched='None'\n",
    "\n",
    "    for n in range(0,N_iter):\n",
    "        #Creating Hamilontian\n",
    "        U_Exp = 1\n",
    "        for i in range(0,N):\n",
    "            U_Exp = torch.tensor(np.kron(U_Exp,id),dtype=dt)#initializing unitary\n",
    "        for m in range(0,M):#Product of pulses\n",
    "            pulse_coef = R[m]\n",
    "            H1 = sum_pauli(pulse_coef[:N],sx) + sum_pauli(pulse_coef[N:],sy)\n",
    "            U_Exp = torch.matmul(torch.matrix_exp(-1j*(H0+H1)*t/M),U_Exp)\n",
    "\n",
    "        #Fidelity calulcation given by Nielsen Paper\n",
    "        fidelity = 0\n",
    "        d = 2**N\n",
    "        for i in range(0,len(SU)):\n",
    "            eps_U = torch.matmul(torch.matmul(U_Exp,SU[i]),(U_Exp.conj().T))\n",
    "            target_U = torch.matmul(torch.matmul(target_gate,(SU[i].conj().T)),(target_gate.conj().T))\n",
    "            tr = torch.trace(torch.matmul(target_U,eps_U))\n",
    "            fidelity = fidelity + tr\n",
    "        fidelity = abs(fidelity + d*d)/(d*d*(d+1))    \n",
    "        infidelity = 1 - fidelity\n",
    "        infidelity_list[n] = infidelity.detach()\n",
    "        infidelity.backward()\n",
    "\n",
    "        #Printing statement\n",
    "        #if (n+1)%20==0: \n",
    "            #print('Itertation ', str(n+1), ' out of ', str(N_iter), 'complete. Avg Infidelity: ', str(infidelity.item()))\n",
    "\n",
    "        #optimizer \n",
    "        if optimizer is not None and scheduler is None:  # Update R\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        elif optimizer is not None and scheduler is not None:\n",
    "            optimizer.step()\n",
    "            if loss_in: \n",
    "                scheduler.step(infidelity)\n",
    "            else: \n",
    "                scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        else:\n",
    "            R.data.sub_(lr*R.grad.data) # using data avoids overwriting tensor object\n",
    "            R.grad.data.zero_()           # and it's respective grad info\n",
    "    \n",
    "    #print('The infidelity of the generated gate is: ' + str(infidelity_list.min().item()))\n",
    "    #return R\n",
    "    return infidelity_list.min().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fidelity_XX(J,B,M,input_gate,t,N_iter):\n",
    "    #!/usr/bin/env python3\n",
    "    # -*- coding: utf-8 -*-\n",
    "    \"\"\"\n",
    "    Created on Mon Aug 16 4:33 2021\n",
    "\n",
    "    @author: Bora & Alex\n",
    "    \"\"\"\n",
    "    #imports\n",
    "\n",
    "    #Pauli Matricies \n",
    "    sx = np.array([[0, 1], [1, 0]])\n",
    "    sy = np.array([[0,-1j],[1j,0]])\n",
    "    sz = np.array([[1, 0], [0, -1]])\n",
    "    id = np.array([[1,0],[0,1]])\n",
    "    \n",
    "    #Function definitions \n",
    "    def zero_mat(N):#Generates matrix of zeros\n",
    "        zero_gate = np.array([[0,0],[0,0]])\n",
    "        init = np.array([[0,0],[0,0]])\n",
    "        if N < 2:\n",
    "            return 1\n",
    "        for i in range(0,N - 1):\n",
    "            zero_gate = torch.tensor(np.kron(zero_gate,init))\n",
    "        return zero_gate\n",
    "    def sum_pauli(coef, gate):#Sums Pauli gates with coefficients \n",
    "        N = len(coef)#number of qubits\n",
    "        total_pauli = zero_mat(N)\n",
    "        #Summing all Z gates\n",
    "        for i in range(0,N):\n",
    "            pauli_temp = 1\n",
    "            for j in range(0,i):\n",
    "                pauli_temp = torch.tensor(np.kron(pauli_temp,id))\n",
    "            pauli_temp = torch.tensor(np.kron(pauli_temp,gate))\n",
    "            for j in range(i+1,N):\n",
    "                pauli_temp = torch.tensor(np.kron(pauli_temp,id))\n",
    "            total_pauli = total_pauli + coef[i]*pauli_temp\n",
    "        return total_pauli\n",
    "\n",
    "    #variable initializations\n",
    "    N = len(B)\n",
    "    torch.manual_seed(9)\n",
    "    dt = torch.cdouble # datatype and precision\n",
    "    infidelity_list=torch.zeros([N_iter,1])\n",
    "\n",
    "    #J coefficients gathering (only if J is in N x N matrix, otherwise set J_coef=J)\n",
    "    J_coef = []\n",
    "    for i in range(0,len(J) - 1):\n",
    "        for j in range(0,len(J) - i - 1):\n",
    "            J_coef.append(J[i,j].item())\n",
    "\n",
    "    #H0 generation\n",
    "    permuts = [1,1]\n",
    "    for i in range(2,N):\n",
    "        permuts.append(0)\n",
    "    permuts = list(set(permutations(permuts,N)))\n",
    "    permuts.sort()\n",
    "    permuts.reverse()#All permutations of ZZ coupling stored as bit arrays\n",
    "    H0 = zero_mat(N)\n",
    "    ##Changed gates ZZ -> XX\n",
    "    for i,u in enumerate(permuts):#summing ZZ permutations and J constants\n",
    "        XX_temp = 1\n",
    "        for p in u:\n",
    "            if p==1:\n",
    "                XX_temp = torch.tensor(np.kron(XX_temp,sx))\n",
    "            else:\n",
    "                XX_temp = torch.tensor(np.kron(XX_temp,id))\n",
    "        H0 = H0 + J_coef[i]*XX_temp\n",
    "\n",
    "    #H0 = H0 + sum_pauli(B,sz)\n",
    "    #print(H0) <- checking if the H0 is a XX gate \n",
    "\n",
    "    #Unitary group generation\n",
    "    SU = []\n",
    "    pauli_int = [1, 2, 3, 4]#eq to [sx,sy,sz,id]\n",
    "    perms = list(product(pauli_int,repeat=N))#all permutations of paulis\n",
    "    for p in perms:#mapping integers to pauli \n",
    "        unitary = 1\n",
    "        for pauli in p:\n",
    "            if pauli == 1:\n",
    "                unitary = torch.tensor(np.kron(unitary,sx),dtype=torch.cdouble)\n",
    "            elif pauli == 2:\n",
    "                unitary = torch.tensor(np.kron(unitary,sy),dtype=torch.cdouble)\n",
    "            elif pauli == 3:\n",
    "                unitary = torch.tensor(np.kron(unitary,sz),dtype=torch.cdouble)\n",
    "            elif pauli == 4:\n",
    "                unitary = torch.tensor(np.kron(unitary,id),dtype=torch.cdouble)\n",
    "        SU.append(unitary)\n",
    "\n",
    "    #These are the coefficients we are optimizing\n",
    "    R = torch.rand([M,2*N], dtype=torch.double) *2*np.pi # Random initialization (between 0 and 2pi)\n",
    "    R.requires_grad = True # set flag so we can backpropagate\n",
    "\n",
    "    #Optimizer settings(can be changed & opttimized)\n",
    "    lr=0.3#learning rate\n",
    "\n",
    "    opt = 'SGD'  # Choose optimizer - ADAM, SGD (typical). ADAMW, ADAMax, Adadelta,  \n",
    "                        # Adagrad, Rprop, RMSprop, ASGD, also valid options.     \n",
    "    sched = 'Plateau'  # Choose learning rate scheduler - Plateau, Exponential (typical), Step\n",
    "    \n",
    "    if opt=='ADAM': optimizer = torch.optim.Adam([R], lr = lr, weight_decay=1e-6)\n",
    "    elif opt=='ADAMW': optimizer = torch.optim.AdamW([R], lr = lr, weight_decay=0.01)\n",
    "    elif opt=='ADAMax': optimizer = torch.optim.Adamax([R], lr = lr, weight_decay=0.01)\n",
    "    elif opt=='RMSprop': optimizer = torch.optim.RMSprop([R], lr = lr, momentum=0.2)\n",
    "    elif opt=='Rprop': optimizer = torch.optim.Rprop([R], lr = lr)\n",
    "    elif opt=='Adadelta': optimizer = torch.optim.Adadelta([R], lr = lr) \n",
    "    elif opt=='Adagrad': optimizer = torch.optim.Adagrad([R], lr = lr)\n",
    "    elif opt=='SGD': optimizer = torch.optim.SGD([R], lr = lr, momentum=0.99, nesterov=True)\n",
    "    elif opt=='ASGD': optimizer = torch.optim.ASGD([R], lr = lr)\n",
    "    else: optimizer=None; opt='None'\n",
    "        \n",
    "    if sched=='Step': scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=N_iter/10, gamma=0.9)\n",
    "    elif sched=='Exponential': scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    elif sched=='Plateau': scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',min_lr=0.03, factor=0.3 , patience= 20 ); loss_in=True; \n",
    "    else: scheduler=None; sched='None'\n",
    "\n",
    "    for n in range(0,N_iter):\n",
    "        #Creating Hamilontian\n",
    "        U_Exp = 1\n",
    "        for i in range(0,N):\n",
    "            U_Exp = torch.tensor(np.kron(U_Exp,id),dtype=dt)#initializing unitary\n",
    "        for m in range(0,M):#Product of pulses\n",
    "            pulse_coef = R[m]\n",
    "            H1 = sum_pauli(pulse_coef[:N],sx) + sum_pauli(pulse_coef[N:],sy)\n",
    "            U_Exp = torch.matmul(torch.matrix_exp(-1j*(H0+H1)*t/M),U_Exp)\n",
    "\n",
    "        #Fidelity calulcation given by Nielsen Paper\n",
    "        fidelity = 0\n",
    "        d = 2**N\n",
    "        for i in range(0,len(SU)):\n",
    "            ideal_U = torch.matmul(torch.matmul(input_gate,SU[i]),(input_gate.conj().T))\n",
    "            target_U = torch.matmul(torch.matmul(U_Exp,SU[i]),(U_Exp.conj().T)) # This is Eps(U) = pulse_gate * pauli * pulse_gate^H\n",
    "            tr = torch.trace(torch.matmul(ideal_U,target_U))\n",
    "            fidelity = fidelity + tr\n",
    "        fidelity = abs(fidelity + d*d)/(d*d*(d+1))    \n",
    "        infidelity = 1 - fidelity\n",
    "        infidelity_list[n] = infidelity.detach()\n",
    "        infidelity.backward()\n",
    "\n",
    "        #Printing statement\n",
    "        #if (n+1)%100==0: \n",
    "            #print('Itertation ', str(n+1), ' out of ', str(N_iter), 'complete. Avg Infidelity: ', str(infidelity.item()))\n",
    "\n",
    "        #optimizer \n",
    "        if optimizer is not None and scheduler is None:  # Update R\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        elif optimizer is not None and scheduler is not None:\n",
    "            optimizer.step()\n",
    "            if loss_in: \n",
    "                scheduler.step(infidelity)\n",
    "            else: \n",
    "                scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        else:\n",
    "            R.data.sub_(lr*R.grad.data) # using data avoids overwriting tensor object\n",
    "            R.grad.data.zero_()           # and it's respective grad info\n",
    "    \n",
    "    #print('The infidelity of the generated gate is: ' + str(infidelity_list.min().item()))\n",
    "    #return R\n",
    "    return infidelity_list.min().item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qutrit Function Defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fidelity_Qutrit(J,B,M,input_gate,t,N_iter):\n",
    "    #!/usr/bin/env python3\n",
    "    # -*- coding: utf-8 -*-\n",
    "    \"\"\"\n",
    "    Created on Mon Aug 16 4:33 2021\n",
    "\n",
    "    @author: Bora & Alex\n",
    "    \"\"\"\n",
    "    #imports\n",
    "\n",
    "    #Pauli Matricies \n",
    "    l1 = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]]) \n",
    "    l2 = np.array([[0,-1j, 0],[1j,0, 0], [0, 0, 0]]) \n",
    "    l3 = np.array([[1, 0, 0], [0, -1, 0], [0, 0, 0]]) \n",
    "    l4 = np.array([[0,0,1],[0,0,0],[1,0,0]])\n",
    "    l5 = np.array([[0,0,-1j],[0,0,0],[1j,0,0]])\n",
    "    l6 = np.array([[0,0,0],[0,0,1],[0,1,0]]) #essentially X for 1->2 transition\n",
    "    l7 = np.array([[0,0,0],[0,0,-1j],[0,1j,0]]) #essentially Y for 1->2 transition\n",
    "    l8 = 1/np.sqrt(3) * np.array([[1,0,0],[0,1,0],[0,0,-2]])\n",
    "    id = np.array([[1, 0, 0],[0, 1, 0],[0, 0, 1]])\n",
    "\n",
    "    sx = np.array([[0,1,0],[1,0,0],[0,0,1]])\n",
    "    sy = np.array([[0,-1j,0],[1j,0,0],[0,0,1]])\n",
    "    sx2 = np.array([[1,0,0],[0,0,1],[0,1,0]])\n",
    "    sy2 = np.array([[1,0,0],[0,0,-1j],[0,1j,0]])\n",
    "    sx02 = np.array([[0,0,1],[0,1,0],[1,0,0]])\n",
    "    sy02 = np.array([[0,0,-1j],[0,1,0],[1j,0,0]])\n",
    "    sz = np.array([[1,0,0],[0,-1,0],[0,0,1]])\n",
    "    \n",
    "    #Function definitions \n",
    "    def zero_mat(N):#Generates matrix of zeros\n",
    "        zero_gate = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
    "        init = zero_gate\n",
    "        if N < 2:\n",
    "            return 1\n",
    "        for i in range(0,N - 1):\n",
    "            zero_gate = torch.tensor(np.kron(zero_gate,init),dtype=torch.cdouble)\n",
    "        return zero_gate\n",
    "    def sum_pauli(coef, gate):#Sums Pauli gates with coefficients \n",
    "        N = len(coef)#number of qubits\n",
    "        total_pauli = zero_mat(N)\n",
    "        #Summing all Z gates\n",
    "        for i in range(0,N):\n",
    "            pauli_temp = 1\n",
    "            for j in range(0,i):\n",
    "                pauli_temp = torch.tensor(np.kron(pauli_temp,id))\n",
    "            pauli_temp = torch.tensor(np.kron(pauli_temp,gate))\n",
    "            for j in range(i+1,N):\n",
    "                pauli_temp = torch.tensor(np.kron(pauli_temp,id))\n",
    "            total_pauli = total_pauli + coef[i]*pauli_temp\n",
    "        #return torch.tensor(total_pauli,dtype=torch.cdouble)\n",
    "        return total_pauli\n",
    "\n",
    "    #variable initializations\n",
    "    N = len(B)\n",
    "    torch.manual_seed(1)\n",
    "    dt = torch.cdouble # datatype and precision\n",
    "    infidelity_list=torch.zeros([N_iter,1])\n",
    "\n",
    "    #J coefficients gathering (only if J is in N x N matrix, otherwise set J_coef=J) <- essentially flattens the array\n",
    "    J_coef = []\n",
    "    for i in range(0,len(J) - 1):\n",
    "        for j in range(0,len(J) - i - 1):\n",
    "            J_coef.append(J[i,j].item())\n",
    "\n",
    "    #H0 generation\n",
    "    permuts = [1,1]\n",
    "    for i in range(2,N):\n",
    "        permuts.append(0)\n",
    "    permuts = list(set(permutations(permuts,N)))\n",
    "    permuts.sort()\n",
    "    permuts.reverse()#All permutations of ZZ coupling stored as bit arrays\n",
    "    H0 = zero_mat(N)\n",
    "    ##Changed gates ZZ -> XX\n",
    "    for i,u in enumerate(permuts):#summing ZZ permutations and J constants\n",
    "        XX_temp = 1\n",
    "        for p in u:\n",
    "            if p==1:\n",
    "                XX_temp = torch.tensor(np.kron(XX_temp,l1))\n",
    "            else:\n",
    "                XX_temp = torch.tensor(np.kron(XX_temp,id))\n",
    "        H0 = H0 + J_coef[i]*XX_temp\n",
    "    #H0 = H0 + sum_pauli(B,sz)\n",
    "\n",
    "    #These are the coefficients we are optimizing\n",
    "    R = torch.rand([M,4*N], dtype=torch.double) *2*np.pi # Random initialization (between 0 and 2pi)\n",
    "    R.requires_grad = True # set flag so we can backpropagate\n",
    "\n",
    "    #Optimizer settings(can be changed & opttimized)\n",
    "    lr=0.3#learning rate\n",
    "\n",
    "    opt = 'SGD'  # Choose optimizer - ADAM, SGD (typical). ADAMW, ADAMax, Adadelta,  \n",
    "                        # Adagrad, Rprop, RMSprop, ASGD, also valid options.     \n",
    "    sched = 'Plateau'  # Choose learning rate scheduler - Plateau, Exponential (typical), Step\n",
    "    \n",
    "    if opt=='ADAM': optimizer = torch.optim.Adam([R], lr = lr, weight_decay=1e-6)\n",
    "    elif opt=='ADAMW': optimizer = torch.optim.AdamW([R], lr = lr, weight_decay=0.01)\n",
    "    elif opt=='ADAMax': optimizer = torch.optim.Adamax([R], lr = lr, weight_decay=0.01)\n",
    "    elif opt=='RMSprop': optimizer = torch.optim.RMSprop([R], lr = lr, momentum=0.2)\n",
    "    elif opt=='Rprop': optimizer = torch.optim.Rprop([R], lr = lr)\n",
    "    elif opt=='Adadelta': optimizer = torch.optim.Adadelta([R], lr = lr) \n",
    "    elif opt=='Adagrad': optimizer = torch.optim.Adagrad([R], lr = lr)\n",
    "    elif opt=='SGD': optimizer = torch.optim.SGD([R], lr = lr, momentum=0.99, nesterov=True)\n",
    "    elif opt=='ASGD': optimizer = torch.optim.ASGD([R], lr = lr)\n",
    "    else: optimizer=None; opt='None'\n",
    "        \n",
    "    if sched=='Step': scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=N_iter/10, gamma=0.9)\n",
    "    elif sched=='Exponential': scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    elif sched=='Plateau': scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',min_lr=0.03, factor=0.3 , patience= 20 ); loss_in=True; \n",
    "    else: scheduler=None; sched='None'\n",
    "\n",
    "    for n in range(0,N_iter):\n",
    "        #Creating Drive Hamilontian\n",
    "        U_Exp = 1\n",
    "        for i in range(0,N):\n",
    "            U_Exp = torch.tensor(np.kron(U_Exp,id),dtype=dt)#initializing unitary\n",
    "        for m in range(0,M):#Product of pulses\n",
    "            pulse_coef = R[m]\n",
    "            #H1 = sum_pauli(pulse_coef[:N],l1) + sum_pauli(pulse_coef[N:2*N],l2) + sum_pauli(pulse_coef[2*N:3*N],l4) + sum_pauli(pulse_coef[3*N:4*N],l5) + sum_pauli(pulse_coef[4*N:5*N],l6) + sum_pauli(pulse_coef[5*N:],l7) \n",
    "            H1 = sum_pauli(pulse_coef[:N],l1) + sum_pauli(pulse_coef[N:2*N],l2) + sum_pauli(pulse_coef[2*N:3*N],l4) \n",
    "            #H1 = sum_pauli(pulse_coef[:N],l1) + sum_pauli(pulse_coef[N:2*N],l2) \n",
    "            U_Exp = torch.matmul(torch.matrix_exp(-1j*(H0+H1)*t/M),U_Exp)\n",
    "        \n",
    "        #Orthonormal Matrix Generation \n",
    "        def Matrix_Basis_Gen(d):\n",
    "            X = np.zeros([d,d])\n",
    "            for i in range(d):\n",
    "                X[(i+1) % d,i] = 1\n",
    "            Z = np.zeros([d,d],dtype=np.complex_)\n",
    "            for i in range(d):\n",
    "                Z[i,i] = np.exp((2*np.pi * 1j * i)/d)\n",
    "            Basis = []\n",
    "            for i in range(d):\n",
    "                for j in range(d):\n",
    "                    Basis.append(torch.tensor(np.matmul(np.linalg.matrix_power(X,i),np.linalg.matrix_power(Z,j)),dtype=torch.cdouble))\n",
    "            return Basis\n",
    "\n",
    "        #Fidelity calulcation given by Nielsen Paper\n",
    "        fidelity = 0\n",
    "        d = 3**N\n",
    "        \n",
    "        for U in Matrix_Basis_Gen(d):\n",
    "            ideal_U = torch.matmul(torch.matmul(input_gate,U.conj().T),(input_gate.conj().T))\n",
    "            target_U = torch.matmul(torch.matmul(U_Exp,U),(U_Exp.conj().T)) # This is Eps(U) = pulse_gate * pauli * pulse_gate^H\n",
    "            tr = torch.trace(torch.matmul(ideal_U,target_U))\n",
    "            fidelity = fidelity + tr \n",
    "        fidelity = abs(fidelity + d**2)/(d**2 *(d+1))    \n",
    "        infidelity = 1 - fidelity\n",
    "        infidelity_list[n] = infidelity.detach()\n",
    "        infidelity.backward()\n",
    "\n",
    "        #Printing statement\n",
    "        if (n+1)%50==0: \n",
    "            print('Itertation ', str(n+1), ' out of ', str(N_iter), 'complete. Avg Infidelity: ', str(infidelity.item()))\n",
    "\n",
    "        #optimizer \n",
    "        if optimizer is not None and scheduler is None:  # Update R\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        elif optimizer is not None and scheduler is not None:\n",
    "            optimizer.step()\n",
    "            if loss_in: \n",
    "                scheduler.step(infidelity)\n",
    "            else: \n",
    "                scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        else:\n",
    "            R.data.sub_(lr*R.grad.data) # using data avoids overwriting tensor object\n",
    "            R.grad.data.zero_()           # and it's respective grad info\n",
    "    \n",
    "    #print('The infidelity of the generated gate is: ' + str(infidelity_list.min().item()))\n",
    "    return R\n",
    "    #return infidelity_list.min().item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Qutrit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fidelity_ml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-9563a379b81f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mTimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mFidelities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfidelity_ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCNOT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFidelities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fidelity_ml' is not defined"
     ]
    }
   ],
   "source": [
    "#graph generation for ZZ coupled Hamiltonain\n",
    "CNOT = torch.tensor([[1,0,0,0],[0,1,0,0],[0,0,0,1],[0,0,1,0]],dtype=torch.cdouble)\n",
    "J = torch.tensor([[1,1],[1,1]])\n",
    "\n",
    "Fidelities = []\n",
    "Times = []\n",
    "for i in range(0,31):\n",
    "    Times.append(i/10)\n",
    "    Fidelities.append(1 - fidelity_ml(J,[1,1],16,CNOT,i/10*np.pi/4,1000))\n",
    "\n",
    "plt.plot(Times,Fidelities)\n",
    "plt.xlabel(\"T/Tmin\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.title(\"2 Qubit CNOT Fidelity for XX coupled H0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fidelity_XX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9a6238d2b099>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mTimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mFidelities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfidelity_XX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCNOT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFidelities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fidelity_XX' is not defined"
     ]
    }
   ],
   "source": [
    "#graph generation for XX coupled Hamiltonain\n",
    "CNOT = torch.tensor([[1,0,0,0],[0,1,0,0],[0,0,0,1],[0,0,1,0]],dtype=torch.cdouble)\n",
    "J = torch.tensor([[1,1],[1,1]])\n",
    "\n",
    "Fidelities = []\n",
    "Times = []\n",
    "for i in range(0,31):\n",
    "    Times.append(i/10)\n",
    "    Fidelities.append(1 - fidelity_XX(J,[1,1],16,CNOT,i/10*np.pi/4,1000))\n",
    "\n",
    "plt.plot(Times,Fidelities)\n",
    "plt.xlabel(\"T/Tmin\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.title(\"2 Qubit CNOT Fidelity for XX coupled H0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph generation for XX coupled Hamiltonain\n",
    "CNOT_qutrit = torch.tensor([[1,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0,0],[0,0,0,1,0,0,0,0,0],[0,0,1,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],\n",
    "[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0]],dtype=torch.cdouble)\n",
    "J = torch.tensor([[1,1],[1,1]])\n",
    "\n",
    "Fidelities = []\n",
    "Times = []\n",
    "for i in range(0,31):\n",
    "    Times.append(i/10)\n",
    "    Fidelities.append(1 - fidelity_XX(J,[1,1],16,CNOT,i/10*np.pi/4,1000))\n",
    "\n",
    "plt.plot(Times,Fidelities)\n",
    "plt.xlabel(\"T/Tmin\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.title(\"2 Qubit CNOT Fidelity for XX coupled H0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNOT_qutrit = torch.tensor([[1,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,1,0,0,0,0],[0,0,0,1,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],\n",
    "[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0]],dtype=torch.cdouble)\n",
    "J = torch.tensor([[1,1],[1,1]])\n",
    "fidelity_Qutrit(J,[1,1],32,CNOT_qutrit,10*np.pi,200)\n",
    "\n",
    "Fidelities = []\n",
    "Times = []\n",
    "for i in range(0,31):\n",
    "    Times.append(i/10)\n",
    "    Fidelities.append(1 - fidelity_Qutrit(J,[1,1],16,CNOT_qutrit,i/10*np.pi/4,500))\n",
    "\n",
    "plt.plot(Times,Fidelities)\n",
    "plt.xlabel(\"T/Tmin\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.title(\"2 Qubit CNOT Fidelity for XX coupled Qutrit H0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itertation  20  out of  1000 complete. Avg Infidelity:  0.8833275397898315\n",
      "Itertation  40  out of  1000 complete. Avg Infidelity:  0.8830224065577897\n",
      "Itertation  60  out of  1000 complete. Avg Infidelity:  0.8825532105317029\n",
      "Itertation  80  out of  1000 complete. Avg Infidelity:  0.881943204055911\n",
      "Itertation  100  out of  1000 complete. Avg Infidelity:  0.8812154639972397\n",
      "Itertation  120  out of  1000 complete. Avg Infidelity:  0.8803944323145223\n",
      "Itertation  140  out of  1000 complete. Avg Infidelity:  0.8795035617404974\n",
      "Itertation  160  out of  1000 complete. Avg Infidelity:  0.8785588480811006\n",
      "Itertation  180  out of  1000 complete. Avg Infidelity:  0.8775600290546578\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e72c9b93731a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m [0,0,0,0,0,0,1,0,0],[0,0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,0,1]],dtype=torch.cdouble)\n\u001b[1;32m      3\u001b[0m \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfidelity_Qutrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIdentity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-fe2ef8b52383>\u001b[0m in \u001b[0;36mfidelity_Qutrit\u001b[0;34m(J, B, M, target_gate, t, N_iter)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0minfidelity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfidelity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0minfidelity_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfidelity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0minfidelity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m#Printing statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py7/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Identity = torch.tensor([[1,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0,0],[0,0,1,0,0,0,0,0,0],[0,0,0,0,1,0,0,0,0],[0,0,0,1,0,0,0,0,0],[0,0,0,0,0,1,0,0,0],\n",
    "[0,0,0,0,0,0,1,0,0],[0,0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,0,1]],dtype=torch.cdouble)\n",
    "J = torch.tensor([[1,1],[1,1]])\n",
    "fidelity_Qutrit(J,[1,1],32,Identity,np.pi/4,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "        [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "        [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
       "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n",
       "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n",
       "        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]],\n",
       "       dtype=torch.complex128)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNOT_qutrit.T * CNOT_qutrit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itertation  20  out of  200 complete. Avg Infidelity:  0.8118579802397738\n",
      "Itertation  40  out of  200 complete. Avg Infidelity:  0.8082186603551773\n",
      "Itertation  60  out of  200 complete. Avg Infidelity:  0.8074939484808253\n",
      "Itertation  80  out of  200 complete. Avg Infidelity:  0.8073040874940538\n",
      "Itertation  100  out of  200 complete. Avg Infidelity:  0.8072776392452026\n",
      "Itertation  120  out of  200 complete. Avg Infidelity:  0.8072856978763698\n",
      "Itertation  140  out of  200 complete. Avg Infidelity:  0.807271588421193\n",
      "Itertation  160  out of  200 complete. Avg Infidelity:  0.80727785596674\n",
      "Itertation  180  out of  200 complete. Avg Infidelity:  0.807272408124901\n",
      "Itertation  200  out of  200 complete. Avg Infidelity:  0.8072729670784975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4276,  5.5662,  2.2977,  0.6540],\n",
       "        [-0.0849,  4.1260,  0.7485,  0.9280],\n",
       "        [ 0.8269,  3.8214,  3.1729,  1.6616],\n",
       "        [ 3.5604,  2.8295,  4.8471,  0.4763],\n",
       "        [ 0.4893,  5.0739,  6.0796,  3.4469],\n",
       "        [ 3.7098,  0.9607,  2.3485,  4.2243],\n",
       "        [ 1.1343,  0.3738,  5.1079,  2.8282],\n",
       "        [ 0.7442,  2.5203,  1.6716,  6.3206],\n",
       "        [ 1.7282,  6.0637,  5.5998,  4.6028],\n",
       "        [-0.2751,  5.9062,  2.6022,  2.0082],\n",
       "        [ 2.1163,  2.2922,  5.6663, -0.3232],\n",
       "        [ 4.1946,  4.5616,  2.1886,  5.3798],\n",
       "        [ 5.4878,  5.1738,  1.9315,  4.9067],\n",
       "        [ 3.3785,  4.4344,  6.9167,  1.7313],\n",
       "        [ 1.6522,  0.2232,  0.3475,  5.7342],\n",
       "        [ 1.3610,  5.8410,  4.2447,  2.0582],\n",
       "        [ 4.1184,  5.3460,  1.3447,  5.4545],\n",
       "        [ 0.6373,  3.5061,  0.9662,  3.8482],\n",
       "        [ 5.1487,  4.1360,  4.6240,  0.9626],\n",
       "        [ 4.9523,  5.1393,  2.4269,  4.4603],\n",
       "        [ 4.7738,  1.0045,  4.8024,  1.3207],\n",
       "        [ 2.8337,  1.3251,  2.2948,  2.8190],\n",
       "        [ 1.4185,  5.3817, -0.2515,  3.1702],\n",
       "        [ 0.3993,  4.8470,  3.1124,  4.9073],\n",
       "        [ 1.5159,  0.7771,  4.0592,  2.8309],\n",
       "        [ 0.3998,  3.6357,  2.3372,  4.8556],\n",
       "        [ 3.1666,  2.8902,  4.6054,  2.4381],\n",
       "        [ 3.7490,  1.7985,  5.1758,  1.6669],\n",
       "        [ 5.0917,  4.1604,  3.8203,  2.6608],\n",
       "        [ 3.4258, -0.4883,  4.5790,  0.9192],\n",
       "        [ 6.1392,  2.3204,  0.3517,  0.9466],\n",
       "        [ 4.0070,  2.1774,  2.7009,  5.9847]], dtype=torch.float64,\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNOT_qutrit = torch.tensor([[1,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0,0],[0,0,1,0,0,0,0,0,0],[0,0,0,1,0,0,0,0,0],[0,0,0,0,1,0,0,0,0],[0,0,0,0,0,1,0,0,0],\n",
    "[0,0,0,0,0,0,1,0,0],[0,0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,0,1]],dtype=torch.cdouble)\n",
    "J = torch.tensor([[1,1],[1,1]])\n",
    "fidelity_Qutrit(J,[1,1],32,CNOT_qutrit,10*np.pi,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a matrix basis that is orthonormal AND unitary. While the Gell-Mann matrices are orthonormal, they are not unitary. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we will apply a phase of i on specific matrices so that we span the complex plane "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will generate these matrices through the method outlined by the Nielsen paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_Gen(d):\n",
    "    X = np.zeros([d,d])\n",
    "    for i in range(d):\n",
    "        X[(i+1) % d,i] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z_Gen(d):\n",
    "    Z = np.zeros([d,d],dtype=np.complex_)\n",
    "    for i in range(d):\n",
    "        Z[i,i] = np.exp((2*np.pi * 1j * i)/d)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Matrix_Basis_Gen(d):\n",
    "    X = np.zeros([d,d])\n",
    "    for i in range(d):\n",
    "        X[(i+1) % d,i] = 1\n",
    "    Z = np.zeros([d,d],dtype=np.complex_)\n",
    "    for i in range(d):\n",
    "        Z[i,i] = np.exp((2*np.pi * 1j * i)/d)\n",
    "    Basis = []\n",
    "    for i in range(d):\n",
    "        for j in range(d):\n",
    "            Basis.append(np.matmul(np.linalg.matrix_power(X,i),np.linalg.matrix_power(Z,j)))\n",
    "    return Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.+0.j, 0.+0.j],\n",
       "        [0.+0.j, 1.+0.j]]),\n",
       " array([[ 1.+0.0000000e+00j,  0.+0.0000000e+00j],\n",
       "        [ 0.+0.0000000e+00j, -1.+1.2246468e-16j]]),\n",
       " array([[0.+0.j, 1.+0.j],\n",
       "        [1.+0.j, 0.+0.j]]),\n",
       " array([[ 0.+0.0000000e+00j, -1.+1.2246468e-16j],\n",
       "        [ 1.+0.0000000e+00j,  0.+0.0000000e+00j]])]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matrix_Basis_Gen(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "efa8cc2a4d6f65357a972a944fbd3e02e547e4ca88e36cc716fc5e5cc822571e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
